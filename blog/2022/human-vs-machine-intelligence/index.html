<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>On human and machine intelligence | Can Bogoclu</title> <meta name="author" content="Can Bogoclu"/> <meta name="description" content="Why some models need more human intelligence than others"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üõ∏</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://canbooo.github.io/blog/2022/human-vs-machine-intelligence/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Can¬†</span>Bogoclu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">On human and machine intelligence</h1> <p class="post-meta">August 27, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> ¬† ¬∑ ¬† <a href="/blog/tag/machinelearning"> <i class="fas fa-hashtag fa-sm"></i> machinelearning</a> ¬† ¬† ¬∑ ¬† <a href="/blog/category/general"> <i class="fas fa-tag fa-sm"></i> general</a> ¬† </p> </header> <article class="post-content"> <h3 id="prelude">Prelude</h3> <p>Since this blog is mainly about machine learning (ML), I thought it would be appropriate to start with a general discussion about various kinds of ML algorithms and their relation to human intelligence. Don‚Äôt worry, I am not a psychologist, nor a neurologist. My knowledge about actual human intelligence is quite limited. What I am referring to as intelligence here has a much more practical meaning. I hate titles in the form ‚ÄúX is all you need‚Äù too, which makes having chosen the following title even more ironic:</p> <h3 id="human-intelligence-is-all-you-need">Human intelligence is all you need!</h3> <p>Consider the following qualitative plot (and beware of any qualitative plots, even the correct ones are always misleading).</p> <div class="col-8 mx-auto"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/humanvsmodelintelligence-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/humanvsmodelintelligence-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/humanvsmodelintelligence-1400.webp"></source> <img src="/assets/img/humanvsmodelintelligence.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> A qualitative plot about the required human intelligence for achieving good results with various machine learning algorithms </div> <p>Notice that the term intelligence is within quotation marks, as the usage here refers to something else than what you might be used to. On the vertical axis, human intelligence describes the amount of ‚Äúwork‚Äù, the human has to conduct to achieve good results with the corresponding methods. Although I am only considering supervised algorithms here, similar arguments can be made about other ML algorithms from various domains.</p> <p>For example, consider the generalized linear model (GLM) and ist special case the linear regression. One needs to extract good features, and has to have some knowledge about the distribution of the target variable as well as its relationship to input features, thus the model family. Although these models are ranked quite low on the horizontal model capacity axis, this does not mean they are bad models. It merely means that the number of different functions are smaller, which are representable with a fixed size of these models. Actually, given sufficient human or user ‚Äòintelligence‚Äô, i.e. correct assumptions, they are the best models and often achieve high and even the best ranks on kaggle competitions (see for example <a href="https://www.kaggle.com/c/covid19-global-forecasting-week-5/discussion/151461" target="_blank" rel="noopener noreferrer">here</a> or <a href="https://www.kaggle.com/code/jkraju/1st-place-solution/script" target="_blank" rel="noopener noreferrer">here</a>). Notice that although the models are linear, some features are polynomial or sinusoidal (i.e. Fourier), which makes these solutions non-linear, but in any case, these simple models can beat more complicated ones, given enough human input.</p> <p>However, these models sometimes require superhuman ‚Äòintelligence‚Äô, as the perfect features or the model family may not be as clear or easy to derive. In this case, we can use the more ‚Äòintelligent‚Äô models, i.e. ones with a higher capacity. However, the more capacity a model has, the more data it will often need. The models in the second category. These models seek to reduce the required human ‚Äòintelligence‚Äô, specifically the exact model family and to some extent features, by some smoothness or local convexity assumptions about the underlying function. These models tend to scale with data to some extent, but they suffer from having a small data set, compared to the linear models.</p> <p>Going up the model capacity scale, we see that the required human ‚Äòintelligence‚Äô decreases along with the assumptions. Multi layer perceptrons (MLP) drops most of the assumptions and increase the capacity greatly compared to the previous group of methods. Nevertheless, the requirements about data as well as other difficulties in training increase also greatly. To overcome some of these problems, various inductive biases are used in structures. Residual networks (ResNet) were revolutionary for their times and they allowed training much deeper models. The skip connections are still used in many architectures. Moreover, long-short term memory (LSTM) and convolutional neural networks (CNN) exploit the locality assumptions about their inputs; they assume that the features that are close to one another are more related to each other. This allows them to learn more meaningful representations compared to feed-forward architectures. Nonetheless, all of these architectures are closely related to MLPs, as all latent dimensions are combined with each others.</p> <p>Finally, there are transformers, which deserve their own category. Most of the current work about large language models use transformers. Their success influenced other domains such as vision to try them out too. They drop the inductive biases, introduced by humans through LSTMs and CNNs. Instead, they try to learn their own inductive biases through the attention mechanism; they neither assume every input feature to be related to every other one as in feed-forward networks, nor they require a locality assumption between the related and relevant inputs. Thus, they reduce the human input compared to previous methods and instead gain the intelligence in a more self-supervised way, even for supervised tasks. However, they require quite a lot of data. This is less of a problem for the language domain, but more of a problem for other domains such as computer vision or tabular data sets. Not only the amount of data, but also the amount of compute makes them quite expensive endeavors. Nonetheless, they often achieve state-of-the-art results for those who can afford to use them. Moreover, they seem to enable a better scaling compared to the previous frameworks.</p> <h3 id="so-what">So what?</h3> <p>Are the transformers the solution to all our problems? Should we abandon linear models, or kernel-based methods entirely? In this post, I tried to convince you otherwise. All of these models have their place and all of them are valuable. I hope to have shown that using the qualitative plot above. There is also an argument about the infamous ‚ÄúNo-free-lunch‚Äù theorem, but most of the modern researchers seem to have (to some extent rightfully) a problem with this argument, so I tried to refrain from it. Nevertheless, knowing about all of these approaches and choosing the most appropriate one should yield the best results and I hope to have given you a rough guide from one perspective about how you can eliminate the inappropriate ones. I hope I could also motivate you to learn about the ones that you have been ignoring until now. Finally, please <a href="https://www.youtube.com/watch?v=TrdevFK_am4&amp;t=1260s" target="_blank" rel="noopener noreferrer">listen to Yannic</a>, my favorite youtuber, talk about why the transformers are valuable but also are not the final solution to all our scaling problems, which also summarizes my thoughts quite well.</p> <p>If you find any typo or other mistake, please feel free to contact me. It would make me very happy to know that someone else is reading these. See the <a href="https://canbooo.github.io/">about page</a> for contact information.</p> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2024 Can Bogoclu. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>