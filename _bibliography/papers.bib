---
---

@InProceedings{Bogoclu2016,
  author      = {Can Bogoclu and Dirk Roos},
  booktitle   = {ECCOMAS},
  title       = {A benchmark of contemporary metamodelling algorithms},
  year        = {2016},
  abstract    = {Growing popularity of probabilistic and stochastic optimization methods in engineering applications has vastly increased the number of required sampling points to obtain a solution. Depending on the complexity of the underlying physical model, this often proves to be a computationally burdensome challenge. In order to overcome this challenge, one possible approach is to use surrogate models, which approximate the responses of the physical model in a given variable subspace. In the past years, many different metamodeling algorithms like Gaussian process (Kriging), moving least squares, radial basis functions, regression neural networks and support vector regression have been proposed as an alternative to ordinary least squares (polynomial regression). The choice of the best metamodeling algorithm for any application is not a trivial task. Although there have been previous attempts to compare at least some of these methods to some extent, there is only a small number of publications comparing all of these algorithms over a large number of multidimensional functions with varying characteristics. In this paper, a comparison of the aforementioned methods is carried out. Using well-known analytical test functions for optimization, this paper aims to shed some light on the question, which algorithm performs best under which conditions. Apart from the structure of analytical test functions, the influence of the number of sampling points and amount of noise in the observations on the performance of metamodeling algorithms is investigated, since these are often two very important constraints regarding industrial applications.},
  bibtex_show = {true},
  doi         = {10.7712/100016.2039.7645},
  owner       = {Bogoclu},
  pdf         = {https://www.researchgate.net/publication/304114701_A_BENCHMARK_OF_CONTEMPORARY_METAMODELING_ALGORITHMS},
  timestamp   = {2016.11.21},
}

@Article{Trapp2019,
  author      = {Maximilian Trapp and Can Bogoclu and Tamara Nestorovi\'{c} and Dirk Roos},
  journal     = {Mechanical Systems and Signal Processing},
  title       = {Intelligent optimization and machine learning algorithms for structural anomaly detection using seismic signals},
  year        = {2019},
  volume      = {133},
  abstract    = {The lack of anomaly detection methods during mechanized tunnelling can cause financial loss and deficits in drilling time. On-site excavation requires hard obstacles to be recognized prior to drilling in order to avoid damaging the tunnel boring machine and to adjust the propagation velocity. The efficiency of the structural anomaly detection can be increased with intelligent optimization techniques and machine learning. In this research, the anomaly in a simple structure is detected by comparing the experimental measurements of the structural vibrations with numerical simulations using parameter estimation methods.},
  bibtex_show = {true},
  doi         = {10.1016/j.ymssp.2019.106250},
  owner       = {Bogoclu},
  pdf         = {https://www.sciencedirect.com/science/article/abs/pii/S0888327019304650},
  timestamp   = {2019.08.03},
}

@Article{Bogoclu2021,
  author      = {Can Bogoclu and Tamara Nestorovi{\'c} and Dirk Roos},
  journal     = {Applied Soft Computing},
  title       = {Local {L}atin hypercube refinement for multi-objective design uncertainty optimization},
  year        = {2021},
  abstract    = {Optimizing the reliability and the robustness of a design is important but often unaffordable due to high sample requirements. Surrogate models based on statistical and machine learning methods are used to increase the sample efficiency. However, for higher dimensional or multi-modal systems, surrogate models may also require a large amount of samples to achieve good results. We propose a sequential sampling strategy for the surrogate based solution of multi-objective reliability based robust design optimization problems. Proposed local Latin hypercube refinement (LoLHR) strategy is model-agnostic and can be combined with any surrogate model because there is no free lunch but possibly a budget one. The proposed method is compared to stationary sampling as well as other proposed strategies from the literature. Gaussian process and support vector regression are both used as surrogate models. Empirical evidence is presented, showing that LoLHR achieves on average better results compared to other surrogate based strategies on the tested examples.},
  arxiv       = {2108.08890},
  bibtex_show = {true},
  doi         = {10.1016/j.asoc.2021.107807},
  pdf         = {https://www.sciencedirect.com/science/article/abs/pii/S1568494621007286},
}

@InProceedings{Bogoclu2017,
  author      = {Can Bogoclu and Dirk Roos},
  booktitle   = {12th International Conference on Structural Safety \& Reliability {ICOSSAR}},
  title       = {Reliability analysis of non-linear and multimodal limit state functions using adaptive {K}riging},
  year        = {2017},
  pages       = {935-943},
  publisher   = {TU Wien Verlag},
  volume      = {1},
  abstract    = {Reliability analysis of non-linear and multimodal limit state functions is a computationally burdensome task, especially if the sampling based strategies are followed. Response surface methods with various adaptive sampling schemes are previously developed to greatly reduce the required number of sample points] but these methods either do not account for the multimodal limit state function or use linear regression based designs of experiments (i.e. D-Optimal) which do not harmonize as well with other response surface methods. We propose combining an unsupervised learning algorithm (DBSCAN) for handling the multimodality with a Latin hypercube design based approach and adapting the process to reliability analysis efficiently. Anisotropic universal Kriging is used for surrogate modeling in combination with directional sampling for limit state functions with a low number of parameters n ≤ 12 and Monte-Carlo method for n > 12.},
  bibtex_show = {true},
  isbn        = {9783903024281},
  owner       = {Bogoclu},
  pdf         = {https://www.researchgate.net/publication/322743350_Reliability_analysis_of_non-linear_and_multimodal_limit_state_functions_using_adaptive_Kriging},
  timestamp   = {2018.01.31},
  url         = {http://shop.tuverlag.at/de/icossar-2017-conference-program},
}

@Article{Laemmle2023,
  author      = {Sven Lämmle and Can Bogoclu and Kevin Cremanns and Dirk Roos},
  journal     = {Computer Methods in Applied Mechanics and Engineering},
  title       = {Gradient and uncertainty enhanced sequential sampling for global fit},
  year        = {2023},
  issn        = {0045-7825},
  pages       = {116226},
  volume      = {415},
  abstract    = {Surrogate models based on machine learning methods have become an important part of modern engineering to replace costly computer simulations. The data used for creating a surrogate model are essential for the model accuracy and often restricted due to cost and time constraints. Adaptive sampling strategies have been shown to reduce the number of samples needed to create an accurate model. This paper proposes a new sampling strategy for global fit called Gradient and Uncertainty Enhanced Sequential Sampling (GUESS). The acquisition function uses two terms: the predictive posterior uncertainty of the surrogate model for exploration of unseen regions and a weighted approximation of the second and higher-order Taylor expansion values for exploitation. Although various sampling strategies have been proposed so far, the selection of a suitable method is not trivial. Therefore, we compared our proposed strategy to 9 adaptive sampling strategies for global surrogate modeling, based on 26 different 1 to 8-dimensional deterministic benchmarks functions. Results show that GUESS achieved on average the highest sample efficiency compared to other surrogate-based strategies on the tested examples. An ablation study considering the behavior of GUESS in higher dimensions and the importance of surrogate choice is also presented.},
  arxiv       = {2310.00110},
  bibtex_show = {true},
  doi         = {10.1016/j.cma.2023.116226},
  keywords    = {Adaptive sampling, Bayesian optimization, Design of experiments, Gaussian process, ANN},
  pdf         = {https://www.sciencedirect.com/science/article/pii/S004578252300350X},
  url         = {https://www.sciencedirect.com/science/article/pii/S004578252300350X},
}

@Article{BogocluDiss,
  author      = {Bogoclu, Can},
  journal     = {Ruhr-Universit\"{a}t Bochum (PhD Thesis)},
  title       = {Local {L}atin hypercube refinement for uncertainty quantification and optimization: {A}ccelerating the surrogate-based solutions using adaptive sampling},
  year        = {2022},
  abstract    = {The design uncertainty is often ignored, although there are various random factors in any
system such as production tolerances, operational conditions and so on. The number of experi-
ments required for the uncertainty quantification and optimization of a design is often so high
that they become inapplicable to complicated systems. Safety factors are used instead for a fast
but implicit assessment of design reliability. Not only are these unintuitive measures, but they
are also uninformative regarding the design robustness. Moreover, safety factors need to be cal-
ibrated for each new design, which requires one or more reliability assessments, thus relocating
the cost of accounting for uncertainty instead of reducing it. Surrogate model-based solutions
are often used to accelerate the design process by reducing the number of required experiments.
Adaptive experiment schemes are proposed to further reduce the required amount of data by
concentrating on the important domains of the approximated responses. However, most of
these methods only consider a specific combination of methods regarding surrogate modelling
and uncertainty quantification. This work presents an adaptive model refinement framework
for the surrogate-based solution of reliability assessment and multi-objective reliability-based
robust design optimization problems, which aims to maintain the freedom of surrogate choice
since there is no single machine learning algorithm, which outperforms others in all problems},
  bibtex_show = {true},
  pdf         = {https://d-nb.info/1268193348/34},
  school      = {Ruhr-Universit\"{a}t Bochum},
  url         = {https://d-nb.info/1268193348/34},
}






@Article{Hochlenert2023,
  author      = {Dirk Hochlenert and Can Bogoclu and Kevin Cremanns and Lars Gierschner and Dominik Ludmann and Mira Mertens and Timo Tromp and Annika Weggen and Hubert Otten},
  journal     = {Journal of Diabetes Science and Technology},
  title       = {Sensor-assisted wound therapy in plantar diabetic foot ulcer treatment: {A} randomized clinical trial},
  year        = {2023},
  abstract    = {Background: Offloading is the cornerstone of treatment of plantar diabetic foot ulcers. It limits mobility with consequent psychological and cardiovascular side effects, and if devices are removed, healing is delayed.Methods:We developed three non-removable techniques with increasing offloading potential (multilayer felt sole, felt-fiberglass sole, or total contact casts with ventral windows) and sensors built within. Smartwatch and web apps displayed pressure, temperature, humidity, and steps. They alerted patients, staff, and a telemedicine center when pressure limits (125 kPa) were exceeded. Patients were advised to walk as much as they had done before the ulcer episode. To evaluate the potential of this intervention, we enrolled 20 ambulatory patients in a randomized clinical trial. The control group used the same offloading and monitoring system, but neither patients nor therapists received any information or warnings.Results:Three patients withdrew consent. The median time to healing of ulcers was significantly shorter in the intervention group compared with controls, 40.5 (95\% confidence interval [CI] = 28-not applicable [NA]) versus 266.0 (95\% CI = 179-NA) days (P = .037), and increasing ulcer area was observed less frequently during study visits (7.9\% vs 29.7\%, P = .033). A reduction of wound area by 50\% was reached at a median of 10.2 (95\% CI = 7.25-NA) versus 19.1 (95\% CI = 13.36-NA) days (P = .2). Participants walked an average of 1875 (SD = 1590) steps per day in intervention group and 1806 (SD = 1391) in the control group.Conclusions:Sensor-assisted wound therapy may allow rapid closure of plantar foot ulcers while maintaining patient’s mobility during ulcer therapy.},
  bibtex_show = {true},
  doi         = {10.1177/19322968231213095},
  pdf         = {https://doi.org/10.1177/19322968231213095},
  url         = {https://doi.org/10.1177/19322968231213095},
}

@InProceedings{Bogoclu2024,
  author      = {Bogoclu, Can and Vosshall, Robert and Cremanns, Kevin and Roos, Dirk},
  booktitle   = {2024 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA)},
  title       = {Deep {G}aussian Covariance Network with Trajectory Sampling for Data-Efficient Policy Search},
  year        = {2024},
  pages       = {1-7},
  abstract    = {Probabilistic world models increase data efficiency of model-based reinforcement learning (MBRL) by guiding the policy with their epistemic uncertainty to improve exploration and acquire new samples. Moreover, the uncertainty-aware learning procedures in probabilistic approaches lead to robust policies that are less sensitive to noisy observations compared to uncertainty-unaware solutions. We propose to combine trajectory sampling and deep {G}aussian covariance network (DGCN) for a data-efficient solution to MBRL problems in an optimal control setting. We compare trajectory sampling with density-based approximation for uncertainty propagation using three different probabilistic world models; {G}aussian processes (GPs), Bayesian neural networks (BNNs), and DGCNs. We provide empirical evidence using four different well-known test environments, that our method improves the sample-efficiency over other combinations of uncertainty propagation methods and probabilistic models. During our tests, we place particular emphasis on the robustness of the learned policies with respect to noisy initial states.},
  arxiv       = {2403.15908},
  bibtex_show = {true},
  doi         = {10.1109/ACDSA59508.2024.10467448},
  keywords    = {Uncertainty;Computational modeling;Neural networks;Optimal control;Reinforcement learning;Probabilistic logic;Data models;Model-based reinforcement learning;uncertainty propagation;Gaussian processes;Bayesian neural networks},
  pdf         = {https://ieeexplore.ieee.org/document/10467448},
}

@Comment{jabref-meta: databaseType:bibtex;}
